{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameter Optimizer\n",
    "========================\n",
    "\n",
    "Optuna-based hyperparameter optimization.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from ..core.config import PipelineConfig\n",
    "from ..models.factory import ModelFactory\n",
    "\n",
    "\n",
    "class HyperparameterOptimizer:\n",
    "    \"\"\"\n",
    "    Hyperparameter optimization using Optuna.\n",
    "    \n",
    "    Features:\n",
    "    - Bayesian optimization\n",
    "    - Early pruning\n",
    "    - Cross-validation based scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: PipelineConfig,\n",
    "        model_factory: ModelFactory,\n",
    "        logger: Optional[logging.Logger] = None\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.model_factory = model_factory\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        self._check_optuna()\n",
    "    \n",
    "    def _check_optuna(self):\n",
    "        \"\"\"Check if Optuna is available.\"\"\"\n",
    "        try:\n",
    "            import optuna\n",
    "            self.optuna = optuna\n",
    "            self.available = True\n",
    "            # Suppress Optuna logs\n",
    "            optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        except ImportError:\n",
    "            self.optuna = None\n",
    "            self.available = False\n",
    "            self.logger.warning(\"âš ï¸ Optuna not installed, optimization disabled\")\n",
    "    \n",
    "    def optimize(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        cv_folds: int = 5,\n",
    "        n_trials: Optional[int] = None,\n",
    "        timeout: Optional[int] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Optimize hyperparameters for a model.\n",
    "        \n",
    "        Returns:\n",
    "            Best parameters and score\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            self.logger.warning(\"Optuna not available, returning default params\")\n",
    "            return {'best_params': {}, 'best_score': 0}\n",
    "        \n",
    "        n_trials = n_trials or self.config.optuna_trials\n",
    "        timeout = timeout or self.config.optuna_timeout\n",
    "        \n",
    "        self.logger.info(f\"ðŸ” Optimizing {model_name} ({n_trials} trials)...\")\n",
    "        \n",
    "        param_space = self.model_factory.get_param_space(model_name)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = self._sample_params(trial, param_space)\n",
    "            \n",
    "            try:\n",
    "                model = self.model_factory.create_model(model_name, params)\n",
    "                scores = cross_val_score(\n",
    "                    model, X_train, y_train,\n",
    "                    cv=cv_folds, scoring='roc_auc_ovr', n_jobs=-1\n",
    "                )\n",
    "                return scores.mean()\n",
    "            except Exception:\n",
    "                return 0.0\n",
    "        \n",
    "        study = self.optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=False)\n",
    "        \n",
    "        self.logger.info(f\"   âœ… Best score: {study.best_value:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_params': study.best_params,\n",
    "            'best_score': study.best_value,\n",
    "            'n_trials': len(study.trials)\n",
    "        }\n",
    "    \n",
    "    def _sample_params(self, trial, param_space: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Sample hyperparameters from search space.\"\"\"\n",
    "        params = {}\n",
    "        \n",
    "        for name, bounds in param_space.items():\n",
    "            if isinstance(bounds, tuple) and len(bounds) == 2:\n",
    "                low, high = bounds\n",
    "                if isinstance(low, int) and isinstance(high, int):\n",
    "                    params[name] = trial.suggest_int(name, low, high)\n",
    "                elif isinstance(low, float) and isinstance(high, float):\n",
    "                    if name in ['learning_rate', 'C']:\n",
    "                        params[name] = trial.suggest_float(name, low, high, log=True)\n",
    "                    else:\n",
    "                        params[name] = trial.suggest_float(name, low, high)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def optimize_all_models(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series\n",
    "    ) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Optimize all available models.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for model_name in self.model_factory.get_available_models():\n",
    "            result = self.optimize(model_name, X_train, y_train)\n",
    "            results[model_name] = result\n",
    "        \n",
    "        return results\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}