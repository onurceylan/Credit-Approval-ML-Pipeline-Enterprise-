{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Evaluator\n",
    "===============\n",
    "\n",
    "Comprehensive model evaluation and comparison.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from ..core.config import PipelineConfig\n",
    "from ..core.exceptions import ModelEvaluationError\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation.\n",
    "    \n",
    "    Features:\n",
    "    - Test set evaluation\n",
    "    - Model comparison\n",
    "    - Best model selection\n",
    "    - Report generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig, logger: Optional[logging.Logger] = None):\n",
    "        self.config = config\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "    \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model: Any,\n",
    "        model_name: str,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.Series\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a single model on test set.\"\"\"\n",
    "        self.logger.info(f\"üìä Evaluating {model_name}...\")\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "            test_roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "        else:\n",
    "            y_pred_proba = None\n",
    "            test_roc_auc = 0.0\n",
    "        \n",
    "        metrics = {\n",
    "            'test_accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "            'test_precision': float(precision_score(y_test, y_pred, average='weighted', zero_division=0)),\n",
    "            'test_recall': float(recall_score(y_test, y_pred, average='weighted', zero_division=0)),\n",
    "            'test_f1': float(f1_score(y_test, y_pred, average='weighted')),\n",
    "            'test_roc_auc': float(test_roc_auc),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
    "            'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"   ‚úÖ Accuracy={metrics['test_accuracy']:.4f}, AUC={metrics['test_roc_auc']:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def evaluate_all(\n",
    "        self,\n",
    "        training_results: Dict[str, Dict],\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.Series\n",
    "    ) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Evaluate all trained models.\"\"\"\n",
    "        self.logger.info(\"\\nüìä Evaluating all models on test set...\")\n",
    "        \n",
    "        evaluation_results = {}\n",
    "        \n",
    "        for model_name, result in training_results.items():\n",
    "            if not result.get('success') or 'model' not in result:\n",
    "                continue\n",
    "            \n",
    "            metrics = self.evaluate_model(result['model'], model_name, X_test, y_test)\n",
    "            evaluation_results[model_name] = metrics\n",
    "        \n",
    "        # Save evaluation report\n",
    "        self._save_report(evaluation_results)\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def select_best_model(\n",
    "        self,\n",
    "        training_results: Dict[str, Dict],\n",
    "        evaluation_results: Dict[str, Dict],\n",
    "        primary_metric: str = 'test_roc_auc'\n",
    "    ) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"Select best model based on metrics.\"\"\"\n",
    "        self.logger.info(\"\\nüèÜ Selecting best model...\")\n",
    "        \n",
    "        model_scores = {}\n",
    "        \n",
    "        for model_name, eval_result in evaluation_results.items():\n",
    "            train_result = training_results.get(model_name, {})\n",
    "            \n",
    "            # Composite score\n",
    "            test_auc = eval_result.get('test_roc_auc', 0)\n",
    "            cv_mean = train_result.get('metrics', {}).get('cv_mean', 0)\n",
    "            cv_std = train_result.get('metrics', {}).get('cv_std', 1)\n",
    "            \n",
    "            stability = 1 / (1 + cv_std)\n",
    "            composite = 0.5 * test_auc + 0.3 * cv_mean + 0.2 * stability\n",
    "            \n",
    "            model_scores[model_name] = {\n",
    "                'composite_score': composite,\n",
    "                'test_roc_auc': test_auc,\n",
    "                'cv_mean': cv_mean,\n",
    "                'cv_std': cv_std\n",
    "            }\n",
    "        \n",
    "        best_model = max(model_scores.keys(), key=lambda k: model_scores[k]['composite_score'])\n",
    "        \n",
    "        self.logger.info(f\"   ü•á Best model: {best_model}\")\n",
    "        self.logger.info(f\"      Composite: {model_scores[best_model]['composite_score']:.4f}\")\n",
    "        self.logger.info(f\"      Test AUC: {model_scores[best_model]['test_roc_auc']:.4f}\")\n",
    "        \n",
    "        return best_model, model_scores\n",
    "    \n",
    "    def _save_report(self, evaluation_results: Dict[str, Dict]):\n",
    "        \"\"\"Save evaluation report.\"\"\"\n",
    "        try:\n",
    "            report_path = Path(self.config.output_dir) / self.config.results_dir / \"evaluation_report.json\"\n",
    "            report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            clean_results = {}\n",
    "            for name, result in evaluation_results.items():\n",
    "                clean_results[name] = {\n",
    "                    k: v for k, v in result.items()\n",
    "                    if k not in ['predictions', 'probabilities']\n",
    "                }\n",
    "            \n",
    "            with open(report_path, 'w') as f:\n",
    "                json.dump(clean_results, f, indent=2, default=str)\n",
    "            \n",
    "            self.logger.info(f\"   üíæ Evaluation report saved\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not save report: {e}\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}