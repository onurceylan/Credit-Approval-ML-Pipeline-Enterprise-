{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metrics and Business Analysis\n",
    "=============================\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ..core.config import PipelineConfig\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Calculates and tracks ML metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger: Optional[logging.Logger] = None):\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        self.metrics_history = []\n",
    "    \n",
    "    def calculate_metrics(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_proba: Optional[np.ndarray] = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate standard classification metrics.\"\"\"\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None:\n",
    "            metrics['roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr')\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def log_metrics(self, model_name: str, metrics: Dict[str, float]):\n",
    "        \"\"\"Log metrics for tracking.\"\"\"\n",
    "        self.metrics_history.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model': model_name,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "\n",
    "class BusinessAnalyzer:\n",
    "    \"\"\"\n",
    "    Business impact analysis.\n",
    "    \n",
    "    Calculates cost-benefit, ROI, and business metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig, logger: Optional[logging.Logger] = None):\n",
    "        self.config = config\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "    \n",
    "    def analyze_impact(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        model_name: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate business impact metrics.\"\"\"\n",
    "        self.logger.info(f\"ðŸ’° Analyzing business impact for {model_name}...\")\n",
    "        \n",
    "        # Calculate confusion matrix elements\n",
    "        is_predicted_bad = y_pred == 1\n",
    "        is_actual_bad = y_true == 1\n",
    "        \n",
    "        tp = np.sum(is_predicted_bad & is_actual_bad)\n",
    "        fp = np.sum(is_predicted_bad & ~is_actual_bad)\n",
    "        tn = np.sum(~is_predicted_bad & ~is_actual_bad)\n",
    "        fn = np.sum(~is_predicted_bad & is_actual_bad)\n",
    "        \n",
    "        # Cost calculations\n",
    "        cost_fp = fp * self.config.cost_false_negative  # Rejected good\n",
    "        cost_fn = fn * self.config.cost_false_positive  # Approved bad\n",
    "        revenue_tn = tn * self.config.revenue_per_approval\n",
    "        \n",
    "        total_cost = cost_fp + cost_fn\n",
    "        total_revenue = revenue_tn\n",
    "        net_profit = total_revenue - total_cost\n",
    "        roi = (net_profit / total_cost * 100) if total_cost > 0 else 0\n",
    "        \n",
    "        impact = {\n",
    "            'true_positives': int(tp),\n",
    "            'false_positives': int(fp),\n",
    "            'true_negatives': int(tn),\n",
    "            'false_negatives': int(fn),\n",
    "            'total_cost': float(total_cost),\n",
    "            'total_revenue': float(total_revenue),\n",
    "            'net_profit': float(net_profit),\n",
    "            'roi_percent': float(roi)\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"   ðŸ’µ Net Profit: ${net_profit:,.0f}, ROI: {roi:.1f}%\")\n",
    "        \n",
    "        return impact\n",
    "    \n",
    "    def generate_business_case(\n",
    "        self,\n",
    "        analysis_results: Dict[str, Dict],\n",
    "        best_model: str\n",
    "    ) -> str:\n",
    "        \"\"\"Generate business case document.\"\"\"\n",
    "        lines = [\n",
    "            \"=\" * 60,\n",
    "            \"BUSINESS CASE DOCUMENT\",\n",
    "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            \"=\" * 60,\n",
    "            \"\",\n",
    "            f\"RECOMMENDED MODEL: {best_model}\",\n",
    "            f\"Expected Net Profit: ${analysis_results[best_model]['net_profit']:,.0f}\",\n",
    "            f\"ROI: {analysis_results[best_model]['roi_percent']:.1f}%\",\n",
    "            \"\",\n",
    "            \"MODEL COMPARISON\",\n",
    "            \"-\" * 40\n",
    "        ]\n",
    "        \n",
    "        for model, impact in sorted(\n",
    "            analysis_results.items(),\n",
    "            key=lambda x: x[1]['net_profit'],\n",
    "            reverse=True\n",
    "        ):\n",
    "            lines.append(f\"{model}: ${impact['net_profit']:,.0f} profit, {impact['roi_percent']:.1f}% ROI\")\n",
    "        \n",
    "        # Save document\n",
    "        doc_path = Path(self.config.output_dir) / self.config.results_dir / \"business_case.txt\"\n",
    "        doc_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        content = '\\n'.join(lines)\n",
    "        with open(doc_path, 'w') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return content\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}