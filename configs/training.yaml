# Training Configuration
# ======================

# Hyperparameter Optimization
optuna:
  n_trials: 50
  timeout: 1800
  sampler: "TPESampler"
  pruner: "MedianPruner"

# Model-specific configurations
models:
  xgboost:
    enabled: true
    params:
      n_estimators: [50, 200]
      max_depth: [3, 8]
      learning_rate: [0.01, 0.3]
      subsample: [0.8, 1.0]
    gpu_params:
      tree_method: "gpu_hist"
      device: "cuda:0"

  lightgbm:
    enabled: true
    params:
      n_estimators: [50, 200]
      max_depth: [3, 10]
      learning_rate: [0.01, 0.3]
      num_leaves: [20, 100]
    gpu_params:
      device: "gpu"
      gpu_platform_id: 0
      gpu_device_id: 0

  catboost:
    enabled: true
    params:
      iterations: [100, 300]
      depth: [4, 8]
      learning_rate: [0.01, 0.3]
      l2_leaf_reg: [1, 10]
    gpu_params:
      task_type: "GPU"
      devices: "0"

  random_forest:
    enabled: true
    params:
      n_estimators: [50, 200]
      max_depth: [5, 25]
      min_samples_split: [2, 20]
      min_samples_leaf: [1, 10]

  gradient_boosting:
    enabled: true
    params:
      n_estimators: [50, 200]
      learning_rate: [0.05, 0.3]
      max_depth: [3, 10]

  logistic_regression:
    enabled: true
    params:
      C: [0.1, 10.0]
      max_iter: 1000

# Cross-validation
cross_validation:
  n_splits: 5
  shuffle: true
  stratified: true

# Early stopping
early_stopping:
  enabled: true
  patience: 50
  min_delta: 0.0001
