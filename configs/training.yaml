# Training Configuration
# ======================

# Hyperparameter Optimization
optuna:
  n_trials: 50
  timeout: 1800
  sampler: "TPESampler"
  pruner: "MedianPruner"

# Model-specific configurations
models:
  xgboost:
    enabled: true
    params:
      n_estimators: [100, 1000]
      max_depth: [3, 15]
      learning_rate: [0.005, 0.3]
      subsample: [0.5, 1.0]
      colsample_bytree: [0.5, 1.0]
      gamma: [0, 10]
      reg_alpha: [0, 10]
      reg_lambda: [0, 10]
    gpu_params:
      tree_method: "gpu_hist"
      device: "cuda:0"

  lightgbm:
    enabled: true
    params:
      n_estimators: [100, 1000]
      max_depth: [-1, 20]
      learning_rate: [0.005, 0.3]
      num_leaves: [20, 256]
      feature_fraction: [0.4, 1.0]
      bagging_fraction: [0.4, 1.0]
      bagging_freq: [1, 7]
    gpu_params:
      device: "gpu"
      gpu_platform_id: 0
      gpu_device_id: 0

  catboost:
    enabled: true
    params:
      iterations: [100, 1000]
      depth: [4, 12]
      learning_rate: [0.005, 0.3]
      l2_leaf_reg: [1, 20]
      random_strength: [0, 10]
    gpu_params:
      task_type: "GPU"
      devices: "0"

  random_forest:
    enabled: true
    params:
      n_estimators: [100, 500]
      max_depth: [5, 50]
      min_samples_split: [2, 30]
      min_samples_leaf: [1, 20]

  gradient_boosting:
    enabled: true
    params:
      n_estimators: [100, 500]
      max_depth: [3, 15]
      learning_rate: [0.01, 0.3]
      subsample: [0.5, 1.0]

  logistic_regression:
    enabled: true
    params:
      C: [0.001, 1000.0]
      max_iter: 1000

# Cross-validation
cross_validation:
  n_splits: 5
  shuffle: true
  stratified: true

# Early stopping
early_stopping:
  enabled: true
  patience: 50
  min_delta: 0.0001
