{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè¶ Enterprise Credit Approval ML Pipeline (V3.5)\n",
                "\n",
                "--- \n",
                "\n",
                "**Clean Architecture + Professional MLOps Hybrid Framework**\n",
                "\n",
                "This notebook runs an end-to-end credit approval system using **modular Python packages (`src/`)**. The project calculates not only technical metrics (AUC/F1) but also critical financial values for enterprise decisions such as **ROI**, **NPV**, and **Amortization**.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ [CELL 1] Environment & Infrastructure Setup\n",
                "\n",
                "In this cell, the Google Colab environment is verified, GPU is detected, and necessary libraries are installed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"üîÑ Checking system...\")\n",
                "\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"üåê Google Colab Environment Detected.\")\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive', force_remount=True)\n",
                "    \n",
                "    # ENTER YOUR PROJECT PATH HERE (Folder name on Drive)\n",
                "    PROJECT_PATH = '/content/drive/MyDrive/credit-approval'\n",
                "    \n",
                "    if os.path.exists(PROJECT_PATH):\n",
                "        os.chdir(PROJECT_PATH)\n",
                "        if PROJECT_PATH not in sys.path:\n",
                "            sys.path.append(PROJECT_PATH)\n",
                "        print(f\"‚úÖ Working directory: {PROJECT_PATH}\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è ERROR: {PROJECT_PATH} not found! Please ensure it is uploaded to Drive.\")\n",
                "else:\n",
                "    print(\"üíª Local Environment Detected.\")\n",
                "    PROJECT_PATH = os.getcwd()\n",
                "    print(f\"‚úÖ Working directory: {PROJECT_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Dependencies\n",
                "if IN_COLAB:\n",
                "    print(\"üì¶ Installing libraries (requirements.txt)...\")\n",
                "    %pip install -r requirements.txt\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è Local run: pip install skipped.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ [CELL 2-8] Enterprise ML Pipeline Execution\n",
                "\n",
                "The pipeline goes through these stages:\n",
                "1. **Data Loading & Validation**\n",
                "2. **Feature Engineering**\n",
                "3. **Model Training & Optimization**\n",
                "4. **Statistical Validation (Friedman Test)**\n",
                "5. **Intelligent Model Selection & Validation**\n",
                "6. **Business Impact Analysis (ROI/NPV)**\n",
                "7. **Automated Offline A/B Simulation** (Inside the pipeline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.core.config import get_config\n",
                "from src.core.logger import setup_logger\n",
                "from src.pipelines.training_pipeline import TrainingPipeline\n",
                "\n",
                "# 1. Load Configuration\n",
                "config = get_config(reload=True)\n",
                "config.optuna_trials = 30  # Reduced trials for fast demo\n",
                "\n",
                "# 2. Prepare Logger\n",
                "logger = setup_logger().logger\n",
                "\n",
                "# 3. Start Pipeline\n",
                "pipeline = TrainingPipeline(config=config, logger=logger)\n",
                "\n",
                "try:\n",
                "    results = pipeline.run()\n",
                "    print(f\"\\nüèÜ PIPELINE COMPLETED! Selected Model: {results['best_model']}\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Execution error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ [STAGE 8] Manual Offline A/B Simulation & Validation\n",
                "\n",
                "This stage simulates A/B testing between the best model (Challenger) and a baseline model (Champion) to validate deployment decisions using bootstrap resampling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.evaluation.ab_testing import ABTestSimulator\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"STAGE 8: A/B TESTING SIMULATION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Variables from pipeline results\n",
                "best_model_name = results['best_model']\n",
                "best_model = results['training_results'][best_model_name]['model']\n",
                "X_train_processed = results['splits']['X_train']\n",
                "y_train = results['splits']['y_train']\n",
                "X_test_processed = results['splits']['X_test']\n",
                "y_test = results['splits']['y_test']\n",
                "output_dir = Path(config.output_dir)\n",
                "plots_dir = output_dir / config.plots_dir\n",
                "\n",
                "print(\"\\nüìä Setting up A/B Test...\")\n",
                "print(f\"   Champion Model: Logistic Regression (Baseline)\")\n",
                "print(f\"   Challenger Model: {best_model_name}\")\n",
                "\n",
                "# Train a simple champion model\n",
                "champion_model = LogisticRegression(random_state=config.random_state, max_iter=1000)\n",
                "champion_model.fit(X_train_processed, y_train)\n",
                "\n",
                "challenger_model = best_model\n",
                "\n",
                "# Initialize A/B Test Simulator\n",
                "ab_simulator = ABTestSimulator(\n",
                "    champion_model=champion_model,\n",
                "    challenger_model=challenger_model,\n",
                "    X_test=X_test_processed,\n",
                "    y_test=y_test,\n",
                "    n_iterations=1000,  # Bootstrap iterations\n",
                "    confidence_level=0.95,  # 95% confidence\n",
                "    random_state=config.random_state,\n",
                "    config=config\n",
                ")\n",
                "\n",
                "# Run A/B Test Simulation\n",
                "ab_results_sim = ab_simulator.run_simulation(\n",
                "    traffic_split=0.5,  # 50/50 split\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "# Generate Report\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"A/B TEST RESULTS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "report = ab_simulator.generate_report(ab_results_sim)\n",
                "print(report)\n",
                "\n",
                "# Save report to file\n",
                "ab_report_txt_path = output_dir / config.results_dir / \"ab_test_report.txt\"\n",
                "with open(ab_report_txt_path, 'w') as f:\n",
                "    f.write(report)\n",
                "\n",
                "print(f\"\\n‚úÖ A/B Test report saved to: {ab_report_txt_path}\")\n",
                "\n",
                "# Visualize Results\n",
                "print(\"\\nüìä Generating A/B Test Dashboard...\")\n",
                "ab_plot_path = plots_dir / \"08_ab_testing_dashboard.png\"\n",
                "ab_simulator.plot_results(ab_results_sim, save_path=str(ab_plot_path))\n",
                "\n",
                "print(f\"‚úÖ A/B Test dashboard saved to: {ab_plot_path}\")\n",
                "\n",
                "# --- DECISION RECOMMENDATION ---\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"DEPLOYMENT DECISION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "if ab_results_sim.winner == 'Challenger':\n",
                "    print(\"‚úÖ RECOMMENDATION: DEPLOY CHALLENGER MODEL\")\n",
                "    print(f\"\\n   Key Improvements:\")\n",
                "    for metric in ['accuracy', 'f1', 'auc']:\n",
                "        improvement = ab_results_sim.statistical_tests[metric]['relative_improvement']\n",
                "        p_value = ab_results_sim.statistical_tests[metric]['p_value']\n",
                "        print(f\"   ‚Ä¢ {metric.upper()}: {improvement:+.2f}% (p={p_value:.6f})\")\n",
                "    \n",
                "    print(f\"\\n   Financial Impact:\")\n",
                "    print(f\"   ‚Ä¢ Annual ROI Increase: ${ab_results_sim.business_impact['annual_financial_impact']:+,.0f}\")\n",
                "    \n",
                "elif ab_results_sim.winner == 'Champion':\n",
                "    print(\"‚ö†Ô∏è  RECOMMENDATION: KEEP CURRENT MODEL\")\n",
                "    print(\"\\n   The baseline model performs equally well or better.\")\n",
                "    \n",
                "else:\n",
                "    print(\"‚öñÔ∏è  RECOMMENDATION: FURTHER INVESTIGATION NEEDED\")\n",
                "    print(\"\\n   No statistically significant difference detected.\")\n",
                "\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(\"\\nüìù Key Metrics for Reporting:\")\n",
                "print(f\"   ‚Ä¢ Simulations Run: {ab_simulator.n_iterations:,}\")\n",
                "print(f\"   ‚Ä¢ Statistical Confidence: {ab_simulator.confidence_level*100:.0f}%\")\n",
                "print(f\"   ‚Ä¢ Effect Size (Cohen's d): {ab_results_sim.effect_size:.4f}\")\n",
                "print(f\"   ‚Ä¢ Winner: {ab_results_sim.winner}\")\n",
                "\n",
                "if ab_results_sim.winner == 'Challenger':\n",
                "    f1_improvement = ab_results_sim.statistical_tests['f1']['relative_improvement']\n",
                "    roi_improvement = ab_results_sim.business_impact['roi_improvement_pct']\n",
                "    print(f\"   ‚Ä¢ F1 Score Improvement: {f1_improvement:+.2f}%\")\n",
                "    print(f\"   ‚Ä¢ ROI Improvement: {roi_improvement:+.2f}%\")\n",
                "\n",
                "print(\"\\n‚úÖ Stage 8 Complete: A/B Testing validated deployment decision!\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ [RESULTS] Advanced Dashboards & Stakeholder Reports\n",
                "\n",
                "Below are the **enterprise analysis dashboards** and reports automatically generated by the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "from pathlib import Path\n",
                "\n",
                "def display_dashboard(title, filename):\n",
                "    path = Path(config.output_dir) / config.plots_dir / filename\n",
                "    if path.exists():\n",
                "        plt.figure(figsize=(15, 12))\n",
                "        img = mpimg.imread(str(path))\n",
                "        plt.imshow(img)\n",
                "        plt.axis('off')\n",
                "        plt.title(title, fontsize=14, fontweight='bold')\n",
                "        plt.show()\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è Visual not found: {filename}\")\n",
                "\n",
                "display_dashboard(\"1. Training Results Dashboard\", \"training_results_dashboard.png\")\n",
                "display_dashboard(\"2. Advanced Model Selection Dashboard\", \"model_selection_dashboard.png\")\n",
                "display_dashboard(\"3. 12-Panel Business Impact Analysis\", \"business_impact_extended.png\")\n",
                "display_dashboard(\"4. Offline A/B Simulation Dashboard\", \"ab_test_dashboard.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìú Executive Summary, Business Case & A/B Results\n",
                "\n",
                "Automatically generated text-based reports and A/B test findings are below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_report(filename):\n",
                "    path = Path(config.output_dir) / config.results_dir / filename\n",
                "    if path.exists():\n",
                "        print(f\"\\n--- [ {filename} ] ---\\n\")\n",
                "        with open(path, 'r', encoding='utf-8') as f:\n",
                "            print(f.read())\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è Report not found: {filename}\")\n",
                "\n",
                "print_report(\"business_case.txt\")\n",
                "print_report(\"ab_test_report.txt\")\n",
                "print_report(\"ab_test_report.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üßπ Memory Optimization\n",
                "\n",
                "You can run the cell below to clear memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gc\n",
                "import torch\n",
                "\n",
                "gc.collect()\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()\n",
                "print(\"üßπ Memory and GPU cache cleared.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}