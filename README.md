# ðŸ“Š Credit Approval ML Pipeline

> **Hybrid MLOps Production Architecture** (Jupyter Notebook + Modular Python)

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Colab Ready](https://img.shields.io/badge/Google_Colab-Ready-orange.svg)](COLAB.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Machine learning pipeline for credit approval prediction featuring statistical validation, comprehensive business impact analysis, and production deployment readiness. This system provides end-to-end ML workflow from data ingestion to stakeholder reporting.

---

## ðŸŒŸ Key Features

- ðŸ¤– **Multi-Algorithm Training**: XGBoost, LightGBM, CatBoost, RandomForest, GradientBoosting, LogisticRegression.
- ðŸ“Š **Statistical Validation**: Friedman test with Bonferroni-corrected post-hoc analysis.
- ðŸ’¼ **Business Impact Analysis**: ROI, NPV (5-Year), and Payback Period calculations.
- ðŸš€ **Production Ready**: Deployment artifacts, modular Python package, and CLI support.
- ðŸ›¡ï¸ **Data Leakage Prevention**: Temporal splitting and comprehensive validation.
- âš¡ **GPU Acceleration**: CUDA support for XGBoost, LightGBM, and CatBoost.
-  **Comprehensive Visualization**: Automated 2x2 Dashboards (Performance, Time, CV, Model Type).

---

## ï¿½ï¸ Architecture & Project Structure

This project follows a **Hybrid MLOps Architecture**, combining the interactivity of Jupyter Notebooks for exploration with the production-grade modularity of Python scripts.

```
credit-approval/
â”‚
â”œâ”€â”€ main.ipynb                    # ðŸ““ INTERACTIVE ENTRY POINT (Google Colab / Jupyter)
â”œâ”€â”€ main.py                       # ðŸ’» CLI ENTRY POINT (Production / Terminal)
â”œâ”€â”€ COLAB.md                      # ðŸ“– Step-by-step Google Colab Guide
â”‚
â”œâ”€â”€ configs/                      # âš™ï¸ Pipeline Configurations (YAML)
â”‚   â”œâ”€â”€ base.yaml                 #    General settings
â”‚   â”œâ”€â”€ training.yaml             #    Model hyperparams & optimization spaces
â”‚   â””â”€â”€ deployment.yaml           #    Business logic & costs
â”‚
â”œâ”€â”€ src/                          # ðŸ“¦ Core Python Package (Modular Logic)
â”‚   â”œâ”€â”€ core/                     #    Config, Logger, Exceptions
â”‚   â”œâ”€â”€ data/                     #    Data Loading & Validation
â”‚   â”œâ”€â”€ features/                 #    Feature Engineering & Preprocessing
â”‚   â”œâ”€â”€ models/                   #    Model Factory (GPU/CPU) & Registry
â”‚   â”œâ”€â”€ training/                 #    Trainer & Optuna Optimizer
â”‚   â”œâ”€â”€ evaluation/               #    Statistical & Financial Evaluators
â”‚   â””â”€â”€ pipelines/                #    End-to-end Pipeline Orchestration
â”‚
â”œâ”€â”€ scripts/                      # ðŸ› ï¸ Task-specific Scripts
â”‚   â”œâ”€â”€ train.py                  #    Standalone training script
â”‚   â””â”€â”€ predict.py                #    Standalone inference script
â”‚
â”œâ”€â”€ tests/                        # ðŸ§ª Unit Tests & Data Quality Checks
â”œâ”€â”€ docker/                       # ðŸ³ Containerization (Dockerfile, Compose)
â”œâ”€â”€ requirements.txt              # ðŸ“‹ Environment Dependencies
â””â”€â”€ setup.py                      # ï¿½ Package Setup (pip install -e .)
```

---

## ï¿½ Output Structure

Execution results are organized into a standardized directory for versioning and reporting.

```
ml_pipeline_output/
â”œâ”€â”€ ðŸ“ models/                    # Serialized models (.joblib)
â”œâ”€â”€ ðŸ“ plots/                     # High-res visualizations (Training Results, ROC, ROI)
â”œâ”€â”€ ðŸ“ results/                   # Structured reports (JSON, Text)
â”‚   â”œâ”€â”€ data_quality_report.json
â”‚   â”œâ”€â”€ training_summary.json
â”‚   â”œâ”€â”€ evaluation_report.json
â”‚   â””â”€â”€ business_case.txt
â””â”€â”€ ðŸ“ logs/                      # Execution trace logs
```

---

## ðŸš€ Quick Start (Google Colab)

The easiest way to run this pipeline is via Google Colab.

1.  Upload the project folder to your Google Drive.
2.  Open `main.ipynb` with Google Colab.
3.  Set Runtime to **T4 GPU** (`Runtime` -> `Change runtime type`).
4.  Follow the instructions in the notebook cells.

See **[COLAB.md](COLAB.md)** for a detailed walkthrough.

---

## ðŸ”¬ Statistical Validation (Friedman Test)

The pipeline implements rigorous statistical testing to compare model performance:

```python
# Friedman test for comparing multiple models across CV folds
statistic, p_value = friedmanchisquare(*cv_matrix)

# Post-hoc pairwise mapping
ranks = rankdata([-m for m in mean_scores])
```

---

## ï¿½ Pipeline Outputs

Upon completion, the pipeline generates rich visualizations:

- **training_results_dashboard.png**: 2x2 Dashboard (Performance, Time, CV Results, Model Types).
- **roc_curves.png**: Comparative ROC curves for all models.
- **business_impact_analysis.png**: Profit vs ROI visualization.
- **feature_importance_[Model].png**: Top predictors for the selected best model.

---

## ðŸ“„ License

MIT License
